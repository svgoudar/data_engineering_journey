{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce321444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb9cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317159 sha256=d055d6199aa5b5887b95168f5aab42eea4c6935d4d2c5be401f1131cd7d764ed\n",
      "  Stored in directory: c:\\users\\test\\appdata\\local\\pip\\cache\\wheels\\9f\\34\\a4\\159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.4.0\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f219727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71989c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "print(findspark.init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79322863",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mvars\u001b[39m(\u001b[43mpyspark\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9ab556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67c91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession is active and ready to use.\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession is active and ready to use.\")\n",
    "else:\n",
    "    print(\"SparkSession is not active. Please create a SparkSession.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da95655a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = range(1,30)\n",
    "# print first element of iterator\n",
    "len(data)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# this will let us know that we created an RDD\n",
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)\n",
    "filteredRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ceb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pyspark==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f776ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os,sys\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON \"] = r\"C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\"\n",
    "\n",
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5a07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a87a4ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Narrow Transformation\n",
    "## Map\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"MapExample\")\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "# mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "# mapped_rdd.collect() # Output: [2, 4, 6, 8, 10]\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "filtered_rdd.collect() # Output: [2, 4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9463b6ee-14ec-487b-bcd4-8221b4fe961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aca7eac-4251-49f4-b55e-1b0ebb2e7e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"UnionExample\")\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "union_rdd.collect() # Output: [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e52d70-8d31-4c16-bd3d-574d468c66c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 7), ('banana', 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wide Transformation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"GroupByExample\")\n",
    "data = [(\"apple\", 2), (\"banana\", 3), (\"apple\", 5), (\"banana\", 1)]\n",
    "rdd = sc.parallelize(data)\n",
    "grouped_rdd = rdd.groupBy(lambda x: x[0])\n",
    "sum_rdd = grouped_rdd.mapValues(lambda values: sum([v[1] for v in values]))\n",
    "sum_rdd.collect() # Output: [('apple', 7), ('banana', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b8c1595-6879-436d-9e76-cbd2e38a3aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "|  Bob| 30|     M|\n",
      "|Diana| 28|     F|\n",
      "+-----+---+------+\n",
      "\n",
      "Folded DataFrame:\n",
      "+-----+---------+\n",
      "| name|(age + 2)|\n",
      "+-----+---------+\n",
      "|  Bob|       32|\n",
      "|Diana|       30|\n",
      "+-----+---------+\n",
      "\n",
      "Pruned DataFrame:\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|  Bob|\n",
      "|Diana|\n",
      "+-----+\n",
      "\n",
      "Reordered Join DataFrame:\n",
      "+-------+---+------+-------------+\n",
      "|   name|age|gender|         city|\n",
      "+-------+---+------+-------------+\n",
      "|  Alice| 25|     F|     New York|\n",
      "|    Bob| 30|     M|San Francisco|\n",
      "|Charlie| 22|     M|  Los Angeles|\n",
      "+-------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"RuleBasedTransformations\").getOrCreate()\n",
    "\n",
    "# Sample input data for DataFrame 1\n",
    "data1 = [(\"Alice\", 25, \"F\"), (\"Bob\", 30, \"M\"), (\"Charlie\", 22, \"M\"), (\"Diana\", 28, \"F\")]\n",
    "\n",
    "# Sample input data for DataFrame 2\n",
    "data2 = [(\"Alice\", \"New York\"), (\"Bob\", \"San Francisco\"), (\"Charlie\", \"Los Angeles\"), (\"Eve\", \"Chicago\")]\n",
    "\n",
    "# Create DataFrames\n",
    "columns1 = [\"name\", \"age\", \"gender\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "columns2 = [\"name\", \"city\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Applying Predicate Pushdown (Filtering)\n",
    "filtered_df = df1.filter(col(\"age\") > 25)\n",
    "\n",
    "# Applying Constant Folding\n",
    "folded_df = filtered_df.select(col(\"name\"), col(\"age\") + 2)\n",
    "\n",
    "# Applying Column Pruning\n",
    "pruned_df = folded_df.select(col(\"name\"))\n",
    "\n",
    "# Join Reordering\n",
    "reordered_join = df1.join(df2, on=\"name\")\n",
    "\n",
    "# Show the final results\n",
    "print(\"Filtered DataFrame:\")\n",
    "filtered_df.show()\n",
    "print(\"Folded DataFrame:\")\n",
    "folded_df.show()\n",
    "print(\"Pruned DataFrame:\")\n",
    "pruned_df.show()\n",
    "print(\"Reordered Join DataFrame:\")\n",
    "reordered_join.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801509ef-f9e4-48e0-8658-ba6d8a350c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
