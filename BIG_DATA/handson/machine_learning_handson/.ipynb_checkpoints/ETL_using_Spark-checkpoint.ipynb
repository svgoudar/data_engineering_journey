{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL using Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color: red'>The purpose of this lab is to show you how to use Spark for ETL jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "    <a href=\"#Objectives\">Objectives\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Datasets\">Datasets\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Setup\">Setup\n",
    "    </a>\n",
    "    <ol>\n",
    "      <li>\n",
    "        <a href=\"#Installing-Required-Libraries\">Installing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "      <li>\n",
    "        <a href=\"#Importing-Required-Libraries\">Importing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li> \n",
    "    <a href=\"#Examples\">Examples\n",
    "    </a>\n",
    "    <ol>\n",
    "    <li>\n",
    "      <a href=\"#Task-1---Create-a-Dataframe-from-the-raw-data-and-write-to-CSV-file.\">Task 1 - Create a Dataframe from the raw data and write to CSV file.\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-2---Read-from-a-csv-file-and-write-to-parquet-file\">Task 2 - Read from a csv file and write to parquet file\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-3---Condense-PARQUET-to-a-single-file.\">Task 3 - Condense PARQUET to a single file.\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-4---Read-from-a-parquet-file-and-write-to-csv-file\">Task 4 - Read from a parquet file and write to csv file\n",
    "      </a>\n",
    "    </li>\n",
    "      </ol>\n",
    "  <li>\n",
    "    <a href=\"#Exercises\">Exercises\n",
    "    </a>\n",
    "  </li>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-1---Extract\">Exercise 1 - Extract\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-2---Transform\">Exercise 2 - Transform\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-3---Load\">Exercise 3 - Load\n",
    "      </a>\n",
    "    </li>\n",
    "  </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    " \n",
    " - Create a Spark Dataframe from the raw data and write to CSV file.\n",
    " - Read from a csv file and write to parquet file\n",
    " - Condense PARQUET to a single file.\n",
    " - Read from a parquet file and write to csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) for connecting to the Spark Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.\n",
    "\n",
    "If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned <a href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/labs/Connecting_to_spark_cluster_using_Skills_Network_labs.ipynb\">here.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pyspark\n",
    "!python3 -m pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Create a Dataframe from the raw data and write to CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of tuples\n",
    "#each tuple contains the student id, height and weight\n",
    "data = [(\"student1\",64,90),\n",
    "        (\"student2\",59,100),\n",
    "        (\"student3\",69,95),\n",
    "        (\"\",70,110),\n",
    "        (\"student5\",60,80),\n",
    "        (\"student3\",69,95),\n",
    "        (\"student6\",62,85),\n",
    "        (\"student7\",65,80),\n",
    "        (\"student7\",65,80)]\n",
    "\n",
    "# some rows are intentionally duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe using createDataFrame and pass the data and the column names.\n",
    "\n",
    "df = spark.createDataFrame(data, [\"student\",\"height_inches\",\"weight_pounds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student1|           64|           90|\n",
      "|student2|           59|          100|\n",
      "|student3|           69|           95|\n",
      "|        |           70|          110|\n",
      "|student5|           60|           80|\n",
      "|student3|           69|           95|\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the data frame\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to csv file\n",
    "\n",
    ">**Note: In Apache Spark, when you use the write method to save a DataFrame to a CSV file, it indeed creates a directory rather than a single file. This is because Spark is designed to run in a distributed manner across multiple nodes, and it saves the output as multiple part files within a directory.The csv file is within the directory.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "|student1|           64|           90|\n",
      "|student2|           59|          100|\n",
      "|student5|           60|           80|\n",
      "|student3|           69|           95|\n",
      "|student3|           69|           95|\n",
      "|    NULL|           70|          110|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v = spark.read.csv(\"student-hw.csv\",header=True)\n",
    "v.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Read from a csv file and write to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "|student1|           64|           90|\n",
      "|student2|           59|          100|\n",
      "|student5|           60|           80|\n",
      "|student3|           69|           95|\n",
      "|student3|           69|           95|\n",
      "|    NULL|           70|          110|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "|    NULL|           70|          110|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that the duplicates are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Observe the rows with null values getting dropped\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do not wish to overwrite use the command df.write.parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the parquet file(s) are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 5E13-3165\n",
      "\n",
      " Directory of C:\\Users\\TEST\\OneDrive\\Documents\\GitHub\\data_engineering_journey\\BIG_DATA\\handson\\machine_learning_handson\\student-hw.parquet\n",
      "\n",
      "29-Jun-24  12:42 PM    <DIR>          .\n",
      "29-Jun-24  12:42 PM    <DIR>          ..\n",
      "29-Jun-24  12:42 PM                20 .part-00000-26174641-0078-4847-b712-525cc033f0e3-c000.snappy.parquet.crc\n",
      "29-Jun-24  12:42 PM                 8 ._SUCCESS.crc\n",
      "29-Jun-24  12:42 PM             1,062 part-00000-26174641-0078-4847-b712-525cc033f0e3-c000.snappy.parquet\n",
      "29-Jun-24  12:42 PM                 0 _SUCCESS\n",
      "               4 File(s)          1,090 bytes\n",
      "               2 Dir(s)  25,002,287,104 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir student-hw.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are a lot of .parquet files in the output.\n",
    "- To improve parallellism, spark stores each dataframe in multiple partitions.\n",
    "- When the data is saved as parquet file, each partition is saved as a separate file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Condense PARQUET to a single file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of partitions in the dataframe to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do not wish to overwrite use the command df.write.parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the parquet file(s) are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 5E13-3165\n",
      "\n",
      " Directory of C:\\Users\\TEST\\OneDrive\\Documents\\GitHub\\data_engineering_journey\\BIG_DATA\\handson\\machine_learning_handson\\student-hw-single.parquet\n",
      "\n",
      "29-Jun-24  12:43 PM    <DIR>          .\n",
      "29-Jun-24  12:43 PM    <DIR>          ..\n",
      "29-Jun-24  12:43 PM                20 .part-00000-9de70335-051e-40be-81ea-2810e951e70a-c000.snappy.parquet.crc\n",
      "29-Jun-24  12:43 PM                 8 ._SUCCESS.crc\n",
      "29-Jun-24  12:43 PM             1,062 part-00000-9de70335-051e-40be-81ea-2810e951e70a-c000.snappy.parquet\n",
      "29-Jun-24  12:43 PM                 0 _SUCCESS\n",
      "               4 File(s)          1,090 bytes\n",
      "               2 Dir(s)  24,999,936,000 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir student-hw-single.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that there is only one .parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Read from a parquet file and write to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert inches to centimeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+------------------+\n",
      "| student|height_inches|weight_pounds|height_centimeters|\n",
      "+--------+-------------+-------------+------------------+\n",
      "|student6|           62|           85|            157.48|\n",
      "|student7|           65|           80|            165.10|\n",
      "|student2|           59|          100|            149.86|\n",
      "|student1|           64|           90|            162.56|\n",
      "|student3|           69|           95|            175.26|\n",
      "|student5|           60|           80|            152.40|\n",
      "+--------+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert inches to centimeters\n",
    "# Multiply the column height_inches with 2.54 to get a new column height_centimeters\n",
    "df = df.withColumn(\"height_centimeters\", expr(\"height_inches * 2.54\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pounds to kilograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+------------------+---------+\n",
      "| student|height_inches|weight_pounds|height_centimeters|weight_kg|\n",
      "+--------+-------------+-------------+------------------+---------+\n",
      "|student6|           62|           85|            157.48|38.555320|\n",
      "|student7|           65|           80|            165.10|36.287360|\n",
      "|student2|           59|          100|            149.86|45.359200|\n",
      "|student1|           64|           90|            162.56|40.823280|\n",
      "|student3|           69|           95|            175.26|43.091240|\n",
      "|student5|           60|           80|            152.40|36.287360|\n",
      "+--------+-------------+-------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pounds to kilograms\n",
    "# Multiply weight_pounds with 0.453592 to get a new column weight_kg\n",
    "df = df.withColumn(\"weight_kg\", expr(\"weight_pounds * 0.453592\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+---------+\n",
      "| student|height_centimeters|weight_kg|\n",
      "+--------+------------------+---------+\n",
      "|student6|            157.48|38.555320|\n",
      "|student7|            165.10|36.287360|\n",
      "|student2|            149.86|45.359200|\n",
      "|student1|            162.56|40.823280|\n",
      "|student3|            175.26|43.091240|\n",
      "|student5|            152.40|36.287360|\n",
      "+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns \"height_inches\",\"weight_pounds\"\n",
    "df = df.drop(\"height_inches\",\"weight_pounds\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "| student|height_cm|weight_kg|\n",
      "+--------+---------+---------+\n",
      "|student6|   157.48|38.555320|\n",
      "|student7|   165.10|36.287360|\n",
      "|student2|   149.86|45.359200|\n",
      "|student1|   162.56|40.823280|\n",
      "|student3|   175.26|43.091240|\n",
      "|student5|   152.40|36.287360|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the lengthy column name \"height_centimeters\" to \"height_cm\"\n",
    "df = df.withColumnRenamed(\"height_centimeters\",\"height_cm\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student_transformed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "| student|height_cm|weight_kg|\n",
      "+--------+---------+---------+\n",
      "|student6|   157.48| 38.55532|\n",
      "|student7|    165.1| 36.28736|\n",
      "|student2|   149.86|  45.3592|\n",
      "|student1|   162.56| 40.82328|\n",
      "|student3|   175.26| 43.09124|\n",
      "|student5|    152.4| 36.28736|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exercises - ETL using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Extract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from student_transformed.csv into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "| student|height_cm|weight_kg|\n",
      "+--------+---------+---------+\n",
      "|student6|   157.48| 38.55532|\n",
      "|student7|    165.1| 36.28736|\n",
      "|student2|   149.86|  45.3592|\n",
      "|student1|   162.56| 40.82328|\n",
      "|student3|   175.26| 43.09124|\n",
      "|student5|    152.4| 36.28736|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use spark.read.csv\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "# display dataframe\n",
    "df.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert cm to meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+\n",
      "| student|height_cm|weight_kg|          height_m|\n",
      "+--------+---------+---------+------------------+\n",
      "|student6|   157.48| 38.55532|            1.5748|\n",
      "|student7|    165.1| 36.28736|             1.651|\n",
      "|student2|   149.86|  45.3592|1.4986000000000002|\n",
      "|student1|   162.56| 40.82328|            1.6256|\n",
      "|student3|   175.26| 43.09124|            1.7526|\n",
      "|student5|    152.4| 36.28736|             1.524|\n",
      "+--------+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr\n",
    "df = df.withColumn(\"height_m\", expr(\"height_cm / 100\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert centimeters to meters\n",
    "# Divide the column height_cm by 100 a new column height_cm\n",
    "df = #TODO\n",
    "# display dataframe\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use df.withColumn() method\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "# Divide the column height_cm by 100 a new column height_cm\n",
    "df = df.withColumn(\"height_meters\", expr(\"height_cm / 100\"))\n",
    "# display dataframe\n",
    "df.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column named bmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+------------------+\n",
      "| student|height_cm|weight_kg|          height_m|               bmi|\n",
      "+--------+---------+---------+------------------+------------------+\n",
      "|student6|   157.48| 38.55532|            1.5748|15.546531093062187|\n",
      "|student7|    165.1| 36.28736|             1.651|13.312549228648752|\n",
      "|student2|   149.86|  45.3592|1.4986000000000002|20.197328530250278|\n",
      "|student1|   162.56| 40.82328|            1.6256|15.448293591899683|\n",
      "|student3|   175.26| 43.09124|            1.7526|14.028892161964118|\n",
      "|student5|    152.4| 36.28736|             1.524|15.623755691955827|\n",
      "+--------+---------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute bmi using the below formula\n",
    "# BMI = weight/(height * height)\n",
    "# weight must be in kgs\n",
    "# height must be in meters\n",
    "df = df.withColumn(\"bmi\", expr(\"weight_kg/(height_m*height_m)\"))\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use df.withColumn() method\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "df = df.withColumn(\"bmi\", expr(\"weight_kg/(height_meters*height_meters)\"))\n",
    "# display dataframe\n",
    "df.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns height_cm, weight_kg and height_meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "| student|          height_m|               bmi|\n",
      "+--------+------------------+------------------+\n",
      "|student6|            1.5748|15.546531093062187|\n",
      "|student7|             1.651|13.312549228648752|\n",
      "|student2|1.4986000000000002|20.197328530250278|\n",
      "|student1|            1.6256|15.448293591899683|\n",
      "|student3|            1.7526|14.028892161964118|\n",
      "|student5|             1.524|15.623755691955827|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns height_cm, weight_kg and height_meters\n",
    "df = df.drop(\"height_cm\",\"weight_kg\",\"height_meters\")\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use df.drop()\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "# Drop the columns height_cm, weight_kg and height_meters\"\n",
    "df = df.drop(\"height_cm\",\"weight_kg\",\"height_meters\")\n",
    "# display dataframe\n",
    "df.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+-----------+\n",
      "| student|          height_m|               bmi|bmi_rounded|\n",
      "+--------+------------------+------------------+-----------+\n",
      "|student6|            1.5748|15.546531093062187|       16.0|\n",
      "|student7|             1.651|13.312549228648752|       13.0|\n",
      "|student2|1.4986000000000002|20.197328530250278|       20.0|\n",
      "|student1|            1.6256|15.448293591899683|       15.0|\n",
      "|student3|            1.7526|14.028892161964118|       14.0|\n",
      "|student5|             1.524|15.623755691955827|       16.0|\n",
      "+--------+------------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us round the column bmi\n",
    "from pyspark.sql.functions import col, round\n",
    "df = df.withColumn(\"bmi_rounded\", round(col(\"bmi\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe into a parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file, set the mode to overwrite\n",
    "df.write.mode(\"overwrite\").parquet(\"new_par.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use df.write\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "df.write.mode(\"overwrite\").parquet(\"student_transformed.parquet\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have completed this lab.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ramesh Sannareddy](https://www.linkedin.com/in/rsannareddy/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork866-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-21|0.1|Ramesh Sannareddy|Initial Version Created|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "prev_pub_hash": "146b5f12ec6d1aa71098f05e4bcc1d2c86607adae3bac6301166ad4e84ef7762"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
