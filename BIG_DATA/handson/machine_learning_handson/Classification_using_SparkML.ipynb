{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using SparkML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color: red'>The purpose of this lab is to show you how to use SparkML to classify varieties of beans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "    <a href=\"#Objectives\">Objectives\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Datasets\">Datasets\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Setup\">Setup\n",
    "    </a>\n",
    "    <ol>\n",
    "      <li>\n",
    "        <a href=\"#Installing-Required-Libraries\">Installing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "      <li>\n",
    "        <a href=\"#Importing-Required-Libraries\">Importing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Examples\">Examples\n",
    "    </a>\n",
    "    <ol>\n",
    "    <li>\n",
    "      <a href=\"#Task-1---Create-a-spark-session\">Task 1 - Create a spark session\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-2---Load-the-data-in-a-csv-file-into-a-dataframe\">Task 2 - Load the data in a csv file into a dataframe\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-3---Identify-the-label-column-and-the-input-columns\">Task 3 - Identify the label column and the input columns\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-4---Split-the-data\">Task 4 - Split the data\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-5---Build-and-Train-a-Logistic-Regression-Model\">Task 5 - Build and Train a Logistic Regression Model\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-6---Evaluate-the-model\">Task 6 - Evaluate the model\n",
    "      </a>\n",
    "    </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Exercises\">Exercises\n",
    "    </a>\n",
    "  </li>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-1---Create-a-spark-session\">Exercise 1 - Create a spark session\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-2---Load-the-data-in-a-csv-file-into-a-dataframe\">Exercise 2 - Load the data in a csv file into a dataframe\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-3---Identify-the-label-column-and-the-input-columns\">Exercise 3 - Identify the label column and the input columns\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-4---Split-the-data\">Exercise 4 - Split the data\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-5---Build-and-Train-a-Logistic-Regression-Model\">Exercise 5 - Build and Train a Logistic Regression Model\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-6---Evaluate-the-model\">Exercise 6 - Evaluate the model\n",
    "      </a>\n",
    "    </li>\n",
    "  </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Use PySpark to connect to a spark cluster.\n",
    " - Create a spark session.\n",
    " - Read a csv file into a data frame.\n",
    " - Split the dataset into training and testing sets.\n",
    " - Use VectorAssembler to combine multiple columns into a single vector column\n",
    " - Use Logistic Regression to build a classification model.\n",
    " - Use metrics to evaluate the model.\n",
    " - Stop the spark session\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In this lab you will be using dataset(s):\n",
    "\n",
    " - Dry Bean dataset. Available at https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset \n",
    " - Modified version of iris dataset. Available at https://archive.ics.uci.edu/ml/datasets/Iris \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) for connecting to the Spark Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.\n",
    "\n",
    "If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned <a href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/labs/Connecting_to_spark_cluster_using_Skills_Network_labs.ipynb\">here.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pyspark\n",
    "!python3 -m pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#import functions/Classes for sparkml\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# import functions/Classes for metrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Create a spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Classification using SparkML\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Load the data in a csv file into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0 2426k    0 16014    0     0   7251      0  0:05:42  0:00:02  0:05:40  7275\n",
      " 10 2426k   10  265k    0     0  85261      0  0:00:29  0:00:03  0:00:26 85470\n",
      " 49 2426k   49 1191k    0     0   283k      0  0:00:08  0:00:04  0:00:04  284k\n",
      "100 2426k  100 2426k    0     0   468k      0  0:00:05  0:00:05 --:--:--  494k\n"
     ]
    }
   ],
   "source": [
    "!curl -O  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/drybeans.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into the spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# using the spark.read.csv function we load the data into a dataframe.\n",
    "# the header = True mentions that there is a header row in out csv file\n",
    "# the inferSchema = True, tells spark to automatically find out the data types of the columns.\n",
    "\n",
    "# Load mpg dataset\n",
    "beans_data = spark.read.csv(\"drybeans.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Area: integer (nullable = true)\n",
      " |-- Perimeter: double (nullable = true)\n",
      " |-- MajorAxisLength: double (nullable = true)\n",
      " |-- MinorAxisLength: double (nullable = true)\n",
      " |-- AspectRation: double (nullable = true)\n",
      " |-- Eccentricity: double (nullable = true)\n",
      " |-- ConvexArea: integer (nullable = true)\n",
      " |-- EquivDiameter: double (nullable = true)\n",
      " |-- Extent: double (nullable = true)\n",
      " |-- Solidity: double (nullable = true)\n",
      " |-- roundness: double (nullable = true)\n",
      " |-- Compactness: double (nullable = true)\n",
      " |-- ShapeFactor1: double (nullable = true)\n",
      " |-- ShapeFactor2: double (nullable = true)\n",
      " |-- ShapeFactor3: double (nullable = true)\n",
      " |-- ShapeFactor4: double (nullable = true)\n",
      " |-- Class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beans_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top 5 rows of selected columns from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+-----------+-----------+-----+\n",
      "| Area|Perimeter|   Solidity|  roundness|Compactness|Class|\n",
      "+-----+---------+-----------+-----------+-----------+-----+\n",
      "|28395|  610.291|0.988855999|0.958027126|0.913357755|SEKER|\n",
      "|28734|  638.018|0.984985603|0.887033637|0.953860842|SEKER|\n",
      "|29380|   624.11|0.989558774|0.947849473|0.908774239|SEKER|\n",
      "|30008|  645.884|0.976695743|0.903936374|0.928328835|SEKER|\n",
      "|30140|  620.134| 0.99089325|0.984877069|0.970515523|SEKER|\n",
      "+-----+---------+-----------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beans_data.select([\"Area\",\"Perimeter\",\"Solidity\",\"roundness\",\"Compactness\",\"Class\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the value counts for the column 'Class'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   Class|count|\n",
      "+--------+-----+\n",
      "|  BOMBAY|  522|\n",
      "|BARBUNYA| 1322|\n",
      "|    CALI| 1630|\n",
      "|   HOROZ| 1928|\n",
      "|   SEKER| 2027|\n",
      "|    SIRA| 2636|\n",
      "|DERMASON| 3546|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beans_data.groupBy('Class').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the string column \"Class\" into a numberic column named \"Label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Class column from string to numerical values\n",
    "indexer = StringIndexer(inputCol=\"Class\", outputCol=\"label\")\n",
    "beans_data = indexer.fit(beans_data).transform(beans_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the value counts for the column 'label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  6.0|  522|\n",
      "|  5.0| 1322|\n",
      "|  4.0| 1630|\n",
      "|  3.0| 1928|\n",
      "|  2.0| 2027|\n",
      "|  1.0| 2636|\n",
      "|  0.0| 3546|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beans_data.groupBy('label').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Identify the label column and the input columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask the VectorAssembler to group a bunch of inputCols as single column named \"features\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use \"Area\",\"Perimeter\",\"Solidity\",\"roundness\",\"Compactness\" as input columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"Area\",\"Perimeter\",\"Solidity\",\"roundness\",\"Compactness\"], outputCol=\"features\")\n",
    "beans_transformed_data = assembler.transform(beans_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the assembled \"features\" and the label column \"MPG\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[28395.0,610.291,...|  2.0|\n",
      "|[28734.0,638.018,...|  2.0|\n",
      "|[29380.0,624.11,0...|  2.0|\n",
      "|[30008.0,645.884,...|  2.0|\n",
      "|[30140.0,620.134,...|  2.0|\n",
      "|[30279.0,634.927,...|  2.0|\n",
      "|[30477.0,670.033,...|  2.0|\n",
      "|[30519.0,629.727,...|  2.0|\n",
      "|[30685.0,635.681,...|  2.0|\n",
      "|[30834.0,631.934,...|  2.0|\n",
      "|[30917.0,640.765,...|  2.0|\n",
      "|[31091.0,638.558,...|  2.0|\n",
      "|[31107.0,640.594,...|  2.0|\n",
      "|[31158.0,642.626,...|  2.0|\n",
      "|[31158.0,641.105,...|  2.0|\n",
      "|[31178.0,636.888,...|  2.0|\n",
      "|[31202.0,644.454,...|  2.0|\n",
      "|[31203.0,639.782,...|  2.0|\n",
      "|[31272.0,638.666,...|  2.0|\n",
      "|[31335.0,635.011,...|  2.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beans_transformed_data.select(\"features\",\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Split the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set in the ratio of 70:30. 70% training data, 30% testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "(training_data, testing_data) = beans_transformed_data.randomSplit([0.7, 0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random_state variable \"seed\" controls the shuffling applied to the data before applying the split. Pass the same integer for reproducible output across multiple function calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Build and Train a Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a LR model and train the model using the training data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore any warnings\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is now trained. We use the testing data to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on testing data\n",
    "predictions = model.transform(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9140055318078953\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy =\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9145127427478639\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print(\"Precision =\", precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.9140055318078953\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print(\"Recall =\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score =  0.9141162063478364\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 score = \", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will use sparkml to classify the iris flower dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Create a spark session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spark session with appname \"Iris Flower Classification\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Class\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use the SparkSession.builder\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.appName(\"Iris Flower Classification\").getOrCreate()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Load the data in a csv file into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  4612  100  4612    0     0   2734      0  0:00:01  0:00:01 --:--:--  2743\n"
     ]
    }
   ],
   "source": [
    "!curl -O  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/iris.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "iris_data = spark.read.csv(\"iris.csv\",header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Use the spark.read.csv function\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "iris_data = spark.read.csv(\"iris.csv\", header=True, inferSchema=True)\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display top 5 rows from dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-----------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+-------------+------------+-------------+------------+-----------+\n",
      "|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Identify the label column and the input columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the string column \"Species\" to a column of numerical values named \"label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Output column label already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m indexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecies\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m iris_data \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43miris_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(iris_data)\n",
      "File \u001b[1;32mC:\\SPARK\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\SPARK\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\SPARK\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\SPARK\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\SPARK\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Output column label already exists."
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "iris_data = indexer.fit(iris_data).transform(iris_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Refer to Task2\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+------------+-----------+-----+-----------------+\n",
      "|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|label|         features|\n",
      "+-------------+------------+-------------+------------+-----------+-----+-----------------+\n",
      "|          5.1|         3.5|          1.4|         0.2|Iris-setosa|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "|          4.9|         3.0|          1.4|         0.2|Iris-setosa|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|          4.7|         3.2|          1.3|         0.2|Iris-setosa|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "|          4.6|         3.1|          1.5|         0.2|Iris-setosa|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|          5.0|         3.6|          1.4|         0.2|Iris-setosa|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "|          5.4|         3.9|          1.7|         0.4|Iris-setosa|  0.0|[5.4,3.9,1.7,0.4]|\n",
      "|          4.6|         3.4|          1.4|         0.3|Iris-setosa|  0.0|[4.6,3.4,1.4,0.3]|\n",
      "|          5.0|         3.4|          1.5|         0.2|Iris-setosa|  0.0|[5.0,3.4,1.5,0.2]|\n",
      "|          4.4|         2.9|          1.4|         0.2|Iris-setosa|  0.0|[4.4,2.9,1.4,0.2]|\n",
      "|          4.9|         3.1|          1.5|         0.1|Iris-setosa|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "|          5.4|         3.7|          1.5|         0.2|Iris-setosa|  0.0|[5.4,3.7,1.5,0.2]|\n",
      "|          4.8|         3.4|          1.6|         0.2|Iris-setosa|  0.0|[4.8,3.4,1.6,0.2]|\n",
      "|          4.8|         3.0|          1.4|         0.1|Iris-setosa|  0.0|[4.8,3.0,1.4,0.1]|\n",
      "|          4.3|         3.0|          1.1|         0.1|Iris-setosa|  0.0|[4.3,3.0,1.1,0.1]|\n",
      "|          5.8|         4.0|          1.2|         0.2|Iris-setosa|  0.0|[5.8,4.0,1.2,0.2]|\n",
      "|          5.7|         4.4|          1.5|         0.4|Iris-setosa|  0.0|[5.7,4.4,1.5,0.4]|\n",
      "|          5.4|         3.9|          1.3|         0.4|Iris-setosa|  0.0|[5.4,3.9,1.3,0.4]|\n",
      "|          5.1|         3.5|          1.4|         0.3|Iris-setosa|  0.0|[5.1,3.5,1.4,0.3]|\n",
      "|          5.7|         3.8|          1.7|         0.3|Iris-setosa|  0.0|[5.7,3.8,1.7,0.3]|\n",
      "|          5.1|         3.8|          1.5|         0.3|Iris-setosa|  0.0|[5.1,3.8,1.5,0.3]|\n",
      "+-------------+------------+-------------+------------+-----------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_transformed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "iris_data = indexer.fit(iris_data).transform(iris_data)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the columns columns \"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\" into a single column named \"features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"],outputCol=\"features\")\n",
    "iris_transformed_data = assembler.transform(iris_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Refer to Task3\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol=\"features\")\n",
    "iris_transformed_data = assembler.transform(iris_data)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the vectorized features and label columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "|[5.4,3.9,1.7,0.4]|  0.0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|\n",
      "|[5.0,3.4,1.5,0.2]|  0.0|\n",
      "|[4.4,2.9,1.4,0.2]|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|\n",
      "|[5.4,3.7,1.5,0.2]|  0.0|\n",
      "|[4.8,3.4,1.6,0.2]|  0.0|\n",
      "|[4.8,3.0,1.4,0.1]|  0.0|\n",
      "|[4.3,3.0,1.1,0.1]|  0.0|\n",
      "|[5.8,4.0,1.2,0.2]|  0.0|\n",
      "|[5.7,4.4,1.5,0.4]|  0.0|\n",
      "|[5.4,3.9,1.3,0.4]|  0.0|\n",
      "|[5.1,3.5,1.4,0.3]|  0.0|\n",
      "|[5.7,3.8,1.7,0.3]|  0.0|\n",
      "|[5.1,3.8,1.5,0.3]|  0.0|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_transformed_data.select(\"features\",\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Split the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing sets in the ratio of 70:30. Use 42 as seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(training_data, testing_data) = iris_transformed_data.randomSplit([0.7,0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "use the randomSplit method\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "(training_data, testing_data) = iris_transformed_data.randomSplit([0.7, 0.3], seed =42)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Build and Train a Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Logistic Regression model and train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "use the fit method of LogisticRegression\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(training_data)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the values using the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 - Evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is now trained. Make the model predict on testing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.transform(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "use the transform method of the model\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "predictions = model.transform(testing_data)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the metrics :\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9782608695652174\n",
      "Precision = 0.9804347826086957\n",
      "Recall = 0.9782608695652174\n",
      "F1 score =  0.978458139351377\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy =\", accuracy)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print(\"Precision =\", precision)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print(\"Recall =\", recall)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 score = \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "use the MulticlassClassificationEvaluator </details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy =\", accuracy)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print(\"Precision =\", precision)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print(\"Recall =\", recall)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 score = \", f1_score)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have completed this lab.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ramesh Sannareddy](https://www.linkedin.com/in/rsannareddy/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork866-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-04-27|0.1|Ramesh Sannareddy|Initial Version Created|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
