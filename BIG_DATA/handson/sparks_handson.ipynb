{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce321444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb9cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.4.0 in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyspark==3.4.0) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\test\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\test\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\test\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.4.0\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f219727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71989c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(findspark\u001b[38;5;241m.\u001b[39minit())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "print(findspark.init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79322863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9ab556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a5ce7",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67c91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession is active and ready to use.\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession is active and ready to use.\")\n",
    "else:\n",
    "    print(\"SparkSession is not active. Please create a SparkSession.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da95655a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = range(1,30)\n",
    "# print first element of iterator\n",
    "len(data)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# this will let us know that we created an RDD\n",
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)\n",
    "filteredRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ceb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pyspark==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f776ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys\n",
    "\n",
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eefa15bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEST\\OneDrive\\Documents\\GitHub\\data_engineering_journey\\BIG_DATA\\handson\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "print(sys.executable)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON \"] = r\"C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15a29f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NamespacePath(['C:\\\\Users\\\\TEST\\\\OneDrive\\\\Documents\\\\GitHub\\\\data_engineering_journey\\\\BIG_DATA\\\\handson\\\\venv\\\\Lib\\\\site-packages\\\\pyspark\\\\python'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark import python\n",
    "print(python.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00dd3d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%PYSPARK_DRIVER_PYTHON%\n"
     ]
    }
   ],
   "source": [
    "!set PYSPARK_DRIVER_PYTHON=C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\n",
    "!echo %PYSPARK_DRIVER_PYTHON%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16478fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'spark.pyspark.python' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'spark.pyspark.driver.python' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!spark.pyspark.python r\"C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\"\n",
    "!spark.pyspark.driver.python r\"C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5a07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\TEST\\AppData\\Local\\Programs\\Python\\Python39\\python3.exe"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
